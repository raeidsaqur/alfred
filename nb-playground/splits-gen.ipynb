{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALFRED DATASET\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, platform\n",
    "import os.path as osp\n",
    "import json\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from typing import *\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# import spacy\n",
    "# from spacy import displacy\n",
    "# import scipy.sparse as sp\n",
    "# import sympy; sympy.init_printing()\n",
    "\n",
    "# import torch\n",
    "# torch.manual_seed(42)\n",
    "\n",
    "from IPython.core.display import display, Image, HTML\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "import json\n",
    "import glob\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Env Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = f\"/mnt/sda4/DATA/ALFRED/data\"\n",
    "PROJ_DIR = f\"{osp.expanduser('~')}/Research/projects/embodied-ai\"\n",
    "SPLITS_DIR = f\"{PROJ_DIR}/alfred/data/splits\"\n",
    "node = platform.node()\n",
    "if (node == 'v') or ('gpu' in node):\n",
    "    print(\"On Vector cluster\")\n",
    "    %env MUJOCO_GL=egl\n",
    "    DATA_DIR = \"/scratch/ssd004/datasets/alfred/data\"\n",
    "    \n",
    "nb_dir = osp.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.insert(0, nb_dir)\n",
    "\n",
    "# print(f\"{os.name}.{platform.system()}.{platform.release()}.{platform.node()}\")\n",
    "# print(f'os.cwd() -> {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/raeidsaqur/Research/projects/embodied-ai/alfred/nb-playground\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splits \n",
    "We want to create a split dealing with just one kind of task for quick debugging and such.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tests_seen': 1533,\n",
      " 'tests_unseen': 1529,\n",
      " 'train': 21023,\n",
      " 'valid_seen': 820,\n",
      " 'valid_unseen': 821}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if not os.path.exists(SPLITS_DIR):\n",
    "    raise FileNotFoundError(\"Splits folder not found\")\n",
    "\n",
    "print_count = lambda x: pprint.pprint({k: len(v) for k, v in x.items()})\n",
    "    \n",
    "# args.splits = 'splits/oct21.json'\n",
    "splits_path = os.path.join(f\"{PROJ_DIR}/alfred/data\", \"splits/oct21.json\" )\n",
    "  \n",
    "with open(splits_path, 'r') as f:\n",
    "    splits = json.load(f)\n",
    "    pprint.pprint({k: len(v) for k, v in splits.items()})\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_TYPES = ['tests_seen',\n",
    "              'tests_unseen',\n",
    "              'train',\n",
    "              'valid_seen',\n",
    "              'valid_unseen']\n",
    "TASK_TYPES = ['pick_and_place_simple', \n",
    "              'pick_clean_then_place_in_recep', \n",
    "              'pick_heat_then_place_in_recep',\n",
    "              'pick_cool_then_place_in_recep', \n",
    "              'pick_two_obj_and_place', \n",
    "              'look_at_obj_in_light',\n",
    "              'pick_and_place_with_movable_recep']\n",
    "\n",
    "# There is 3487/3 trial (unique task_ids) for pick_and_place_simple in train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3245\n",
      "len(v) = 21023\n",
      "No. of pick_and_place_simple samples in train = 3245\n",
      "len(v) = 820\n",
      "No. of pick_and_place_simple samples in valid_seen = 142\n",
      "len(v) = 821\n",
      "No. of pick_and_place_simple samples in valid_unseen = 100\n"
     ]
    }
   ],
   "source": [
    "# splits.update({'tests_seen': [], \n",
    "#               'tests_unseen': []})\n",
    "# s_train = splits['train']\n",
    "# print(f\"len(s_train)={len(s_train)}\")\n",
    "pps = list(filter(lambda d: 'pick_and_place_simple' in d['task'], splits['train']))\n",
    "print(len(pps))\n",
    "\n",
    "splits_pps_fn = 'oct21-pps.json'\n",
    "splits_pps = {}\n",
    "task_name = 'pick_and_place_simple'\n",
    "for k,v in splits.items():\n",
    "        if 'tests' in k:\n",
    "            continue\n",
    "        print(f\"len(v) = {len(v)}\")\n",
    "        #pps = list(filter(lambda d: 'pick_and_place_simple' in d['task'], v))\n",
    "        pps = list(filter(lambda d: task_name in d['task'], v))\n",
    "        print(f\"No. of {task_name} samples in {k} = {len(pps)}\")\n",
    "        splits_pps[k] = pps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 3245, 'valid_seen': 142, 'valid_unseen': 100}\n"
     ]
    }
   ],
   "source": [
    "print_count(splits_pps)\n",
    "\n",
    "taskids_pps = set()\n",
    "t = 'pick_cool_then_place_in_recep-LettuceSliced-None-DiningTable-17/trial_T20190909_070538_437648'\n",
    "# def get_task_id(t):\n",
    "#     return t.split(\"/\")[-1]\n",
    "\n",
    "get_task_id = lambda t: t.split(\"/\")[-1]\n",
    "\n",
    "## Get the unique taskids in train and validation sets corresponding to the pps task\n",
    "for k,v in splits_pps.items():\n",
    "    for d in v:\n",
    "        taskids_pps.add(get_task_id(d['task']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1110\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['trial_T20190909_091246_807206',\n",
       " 'trial_T20190906_213417_095299',\n",
       " 'trial_T20190907_092230_655414',\n",
       " 'trial_T20190909_061144_774666',\n",
       " 'trial_T20190907_173922_758000']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(taskids_pps))\n",
    "import random\n",
    "[v for i,v in enumerate(random.sample(taskids_pps, 5))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tests_seen': 1533,\n",
      " 'tests_unseen': 1529,\n",
      " 'train': 21023,\n",
      " 'valid_seen': 820,\n",
      " 'valid_unseen': 821}\n"
     ]
    }
   ],
   "source": [
    "print_count(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(v) = 1533\n",
      "No. of pick_and_place_simple samples in tests_seen = 0\n",
      "len(v) = 1529\n",
      "No. of pick_and_place_simple samples in tests_unseen = 0\n",
      "{'tests_seen': 0,\n",
      " 'tests_unseen': 0,\n",
      " 'train': 3245,\n",
      " 'valid_seen': 142,\n",
      " 'valid_unseen': 100}\n"
     ]
    }
   ],
   "source": [
    "## Any overlapping task_ids in 'tests' splits from train and valid? No\n",
    "for k,v in splits.items():\n",
    "    if 'tests' not in k:\n",
    "        continue\n",
    "    print(f\"len(v) = {len(v)}\")  \n",
    "    pps = list(filter(lambda d: d['task'] in taskids_pps, v))\n",
    "    print(f\"No. of {task_name} samples in {k} = {len(pps)}\")\n",
    "    splits_pps[k] = pps\n",
    "    \n",
    "print_count(splits_pps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `tests_[seen|unseen]`, only the `traj_data.json` file is provided. No additonal image info is provided, so can't curate tests by task type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coalesced function\n",
    "\n",
    "def get_splits_by_task_type(task_type:str,\n",
    "                            proj_dir=None,\n",
    "                            proj_name='alf', default_split=\"splits/oct21.json\",\n",
    "                            save_json=False,\n",
    "                            debug=False) -> Dict:\n",
    "    print(f\"Getting splits by task: {task_type}\")\n",
    "    if not proj_dir:\n",
    "        proj_dir = f\"{os.path.expanduser('~')}/Research/projects/embodied-ai/{proj_name}\"\n",
    "        if not os.path.exists(proj_dir):\n",
    "            raise FileNotFoundError(\"Project dir does not exist\")\n",
    "\n",
    "    splits_path = os.path.join(f\"{proj_dir}/data\", default_split)\n",
    "    with open(splits_path, 'r') as f:\n",
    "        splits = json.load(f)\n",
    "        if debug: print_count(splits)\n",
    "    splits_task = {}\n",
    "    for k,v in splits.items():\n",
    "        if 'tests' in k:\n",
    "            continue\n",
    "        tvs = list(filter(lambda d: task_type in d['task'], v))\n",
    "        if debug: print(f\"No. of {task_type} samples in {k} = {len(tvs)}\")\n",
    "        splits_task[k] = tvs\n",
    "\n",
    "    for stype in SPLIT_TYPES:\n",
    "        # Set empty list for 'tests_[seen|unseen]'\n",
    "        if stype not in splits_task.keys():\n",
    "            splits_task[stype] = []\n",
    "\n",
    "    if debug: print_count(splits_task)\n",
    "\n",
    "    if save_json:\n",
    "        fn_prefix = default_split.split(\"/\")[-1].split('.')[0]\n",
    "        fn = \"{}-{}.json\".format(fn_prefix, task_type)\n",
    "        save_path = os.path.join(proj_dir, f\"data/splits/{fn}\")\n",
    "        print(f\"\\tSaving {task_type} split to: {save_path}\")\n",
    "        with open(save_path, 'w') as fp:\n",
    "            json.dump(splits_task, fp, sort_keys=True, indent=4)\n",
    "\n",
    "    return splits_task\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alfred_env",
   "language": "python",
   "name": "alfred_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
